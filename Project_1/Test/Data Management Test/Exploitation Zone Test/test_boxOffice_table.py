import unittest
from unittest.mock import patch, MagicMock
from pathlib import Path
from delta import configure_spark_with_delta_pip
from pyspark.sql import SparkSession
import shutil

import os
import sys
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../../../Python/Data_Management/Exploitation_Zone')))
from boxOffice_table import boxOffice_table  # type: ignore


class TestBoxOfficeTable(unittest.TestCase):
    @classmethod
    def setUpClass(cls):
        builder = (
        SparkSession.builder
        .master("local[1]")
        .appName("TestBoxOfficeTable")
        .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
        .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")
        )
        cls.spark = configure_spark_with_delta_pip(builder).getOrCreate()

        cls.test_exploitation_path = Path("test_exploitation_zone/boxoffice")
        cls.test_trusted_zone_path = Path("test_trusted_zone/boxoffice")
        cls.test_trusted_zone_path.mkdir(parents=True, exist_ok=True)

        # Sample DataFrame for mocking
        cls.mock_df = cls.spark.createDataFrame([
            ("10 wins", "John Doe", "Studio A", ["Drama"], "link.jpg", "English", "2022-01-01", "Sample Title", 8.5, "1,234"),
        ], ["Awards", "Director", "Distributor", "Genre", "Poster", "Language", "Released", "Title", "imdbRating", "imdbVotes"])

        # Save to delta format to simulate real file (optional, for local debugging)
        cls.mock_df.write.format("delta").mode("overwrite").save(str(cls.test_trusted_zone_path / "boxoffice_movie_data"))

    @classmethod
    def tearDownClass(cls):
        cls.spark.stop()
        shutil.rmtree("test_exploitation_zone", ignore_errors=True)
        shutil.rmtree("test_trusted_zone", ignore_errors=True)

    @patch("boxOffice_table.load_delta_tables")
    def test_boxoffice_table(self, mock_load_delta_tables):
        mock_load_delta_tables.return_value = {
            "boxoffice_movie_data": self.mock_df
        }

        # Call the function
        boxOffice_table(
            spark=self.spark,
            boxoffice_delta_path=str(self.test_trusted_zone_path),
            exploitation_zone_base="test_exploitation_zone"
        )

        # Check output
        output_path = Path("test_exploitation_zone/boxoffice/boxoffice_movie_data")
        self.assertTrue(output_path.exists())

        # Read and validate the written delta table
        result_df = self.spark.read.format("delta").load(str(output_path))
        self.assertEqual(result_df.count(), 1)
        self.assertEqual(result_df.columns, self.mock_df.columns)

if __name__ == "__main__":
    unittest.main()
