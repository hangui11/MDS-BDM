import os
from pathlib import Path
from delta import *
from pyspark.sql.functions import col, avg, count, when, round
import sys
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from utils import *
from datetime import datetime

def imbd_principals_names_ratings_tables_join(principals_df, names_df, ratings_df):
    # Join people to their movies, then to ratings
    people_movies = (
        principals_df
        .join(ratings_df, principals_df.tconst == ratings_df.tconst, "inner")
        .join(names_df, principals_df.nconst == names_df.nconst, "inner")
    )

    # Remove ambiguity: select and alias all relevant columns
    people_movies = people_movies.select(
        principals_df.nconst.alias("id_person"),
        names_df.primaryName.alias("person_name"),
        principals_df.category,
        ratings_df.averageRating.cast("float").alias("average_rating"),
        principals_df.tconst.alias("id_title")
    )

    return people_movies


def imbd_compute_people_movies_kpis(people_movies):
    kpi_people = (
        people_movies
        .groupBy("id_person", "person_name", "category")
        .agg(
            avg("average_rating").alias("avg_rating"),
            count("id_title").alias("num_titles")
        )
        .orderBy(col("avg_rating").desc())
    )
    return kpi_people

def imbd_titles_episodes_ratings_tables_join(titles_df, episodes_df, ratings_df):
    current_year = datetime.now().year
    titles = titles_df.alias("titles")
    episodes = episodes_df.alias("episodes")
    ratings = ratings_df.alias("ratings")

    joined_table = (
        titles
        .join(episodes, titles.tconst == episodes.tconst, "inner")
        .join(ratings, titles.tconst == ratings.tconst, "left")
    )

    joined_table = (
        joined_table
        .select(
            titles.tconst.alias("id"),
            titles.originalTitle,
            titles.isAdult,
            titles.startYear,
            titles.genres,
            episodes.seasonNumber.cast("int"),
            episodes.episodeNumber.cast("int"),
            ratings.averageRating.cast("float"),
            ratings.numVotes.cast("int"),
        )
        .filter(col("startYear").cast("int") <= current_year)
    )

    return joined_table

def imbd_compute_episodes_ratings_kpis(episode_ratings_df, avg_all_movies):
    episode_ratings_kpi = (
        episode_ratings_df
        .withColumn("total_episodes", col("seasonNumber") * col("episodeNumber"))
        .withColumn("trend", when(col("averageRating") > avg_all_movies, True).otherwise(False))
        .withColumn("score", col("averageRating") * col("numVotes"))
        .withColumn("score", round(col("score"), 2))
    )

    return episode_ratings_kpi


def imbd_tables(spark, imbd_delta_path='../../../Data Management/Trusted Zone/imbd/',
                exploitation_zone_base='../../../Data Management/Exploitation Zone/'):
    

    # spark = get_spark_session()
    exploitation_zone_base = Path(exploitation_zone_base)
    exploitation_zone_base.mkdir(parents=True, exist_ok=True)

    imbd_tables = load_delta_tables(imbd_delta_path, spark)

    principals_df = imbd_tables['imbd_title_principals']
    names_df = imbd_tables['imbd_name_basics']
    ratings_df = imbd_tables['imbd_title_ratings']
    episodes_df = imbd_tables['imbd_title_episode']
    titles_df = imbd_tables['imbd_title_basics']
    
    people_movies_df = imbd_principals_names_ratings_tables_join(principals_df, names_df, ratings_df)
    people_movies_kpi_df = imbd_compute_people_movies_kpis(people_movies_df)
        
    
    avg_all_movies = ratings_df.select(col("averageRating").cast("float")).agg(avg("averageRating")).collect()[0][0]
    episode_ratings_df = imbd_titles_episodes_ratings_tables_join(titles_df, episodes_df, ratings_df)
    episode_ratings_kpi_df = imbd_compute_episodes_ratings_kpis(episode_ratings_df, avg_all_movies)

    imbd_exploitation_path = Path(exploitation_zone_base) / 'imbd'
    imbd_exploitation_path.mkdir(parents=True, exist_ok=True)
    people_movies_kpi_df.write.format("delta").mode("overwrite").save(str(imbd_exploitation_path / 'people_movies_kpi'))
    episode_ratings_kpi_df.write.format("delta").mode("overwrite").save(str(imbd_exploitation_path / 'movie_episode_ratings_kpi'))

    # spark.stop()

# imbd_tables()




